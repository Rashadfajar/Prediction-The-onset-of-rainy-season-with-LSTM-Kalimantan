import numpy as np
import xarray as xr
import keras as ks
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from math import sqrt
import tensorflow as tf
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
from sklearn.model_selection import KFold
from keras.callbacks import EarlyStopping
from keras.regularizers import l2


# Load data
ds = xr.open_dataset("CFSv2Januari.nc", decode_times=False)

# Set global seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Extract variables
var1 = ds['Agustus'].values
var2 = ds['September'].values
var3 = ds['Oktober'].values
var14 = ds['AMH'].values

# Mask NaN values in var14
mask_nan_var14 = np.isnan(var14)

# Apply mask to the other variables
var1[mask_nan_var14] = np.nan
var2[mask_nan_var14] = np.nan
var3[mask_nan_var14] = np.nan

# Fill NaN values with median
median_valuejuni = np.nanmedian(var1)
median_valuejuli = np.nanmedian(var2)
median_valueagustus = np.nanmedian(var3)
median_valueamh = np.nanmedian(var14)

var1[np.isnan(var1)] = median_valuejuni
var2[np.isnan(var2)] = median_valuejuli
var3[np.isnan(var3)] = median_valueagustus
var14[np.isnan(var14)] = median_valueamh


data = np.stack([var1, var2, var3], axis=-1)


# Split data into predictors and target
predictors = data.reshape(data.shape[0]*data.shape[3],data.shape[1], data.shape[2], 1)  # All variables except the last one
target = var14

# Normalisasi data prediktor dan target
scaler_x = MinMaxScaler()
data_reshaped = predictors.reshape(-1, predictors.shape[-1])
data_normalized = scaler_x.fit_transform(data_reshaped).reshape(predictors.shape)

scaler_y = MinMaxScaler()
target_reshaped = target.reshape(-1, 1)
target_normalized = scaler_y.fit_transform(target_reshaped).reshape(target.shape)

# Apply PCA to predictors
n_components = 0.97
pca = PCA(n_components=n_components)
data_flattened = data_normalized.reshape(-1, data_normalized.shape[3])
data_pca = pca.fit_transform(data_flattened)
data_pca = data_pca.reshape(data_normalized.shape[0], data_normalized.shape[1], data_normalized.shape[2],-1)

data_pca= data_pca[:90,:,:,:]
target_normalized=target_normalized[:30,:,:]
data_pca= data_pca.reshape(-1, 3, data_pca.shape[3])
target_normalized = target_normalized.reshape(-1)


# Define the optimal model configuration
layer_config = (8, 16)
dropout_rate = 0.5
learning_rate = 0.001
dense_size = 128
batch_size = 1024

# Function to create model
def create_model():
    model = ks.Sequential([
        ks.layers.Input(shape=(data_pca.shape[1], data_pca.shape[2])),  # Specify the input shape using Input
        ks.layers.LSTM(layer_config[0], activation='tanh', return_sequences=True),
        ks.layers.BatchNormalization(),
        ks.layers.Dropout(dropout_rate),
        ks.layers.LSTM(layer_config[1], activation='tanh', return_sequences=False),
        ks.layers.BatchNormalization(),
        ks.layers.Dropout(dropout_rate),
        ks.layers.Dense(dense_size, activation='relu', kernel_regularizer=l2(0.1)),
        ks.layers.Dense(1, activation='sigmoid')
    ])   
    model.compile(optimizer=ks.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])
    return model

# KFold cross-validation
kf = KFold(n_splits=5, shuffle=False)
all_predictions = []
rmses = []
corrs = []
r2s = []

# Perform KFold cross-validation
for train_index, test_index in kf.split(data_pca):
    X_train, X_test = data_pca[train_index], data_pca[test_index]
    y_train, y_test = target_normalized[train_index], target_normalized[test_index]

    # Create and fit the model
    model = create_model()
    early_stopping = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    History = model.fit(X_train, y_train, batch_size=batch_size, epochs=300, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=0)

    # Prediction
    y_pred = model.predict(X_test)
    y_pred = np.squeeze(y_pred)  
    all_predictions.extend(y_pred)

# Konversi list all_predictions ke numpy array untuk reshape
all_predictions_array = np.array(all_predictions)
